{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "from dataset.latent_dataset import LatentDataset\n",
    "from dataset.latent_image_dataset import LatentImageDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data import Subset\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel.distributed import DistributedDataParallel as DDP\n",
    "import socket\n",
    "from mpi4py import MPI\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_wandb(config):\n",
    "    wandb.init(project=\"Face-diffusion\", entity=\"megleczmate\", sync_tensorboard=True, tags=[\"latent_classifier\"])\n",
    "    # get current time\n",
    "    current_time = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "    wandb.run.name = config['task_name'] + current_time\n",
    "    wandb.run.save()\n",
    "    wandb.config.update(config)\n",
    "    return wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, condition_config, world_size, rank, num_epochs=25, batch_size=64, wandb=None):\n",
    "    # Path to CelebA dataset\n",
    "    data_dir = '/mnt/g/data/latents/700/processed/big_nose'\n",
    "\n",
    "    if rank == 0:\n",
    "        wandb = setup_wandb(condition_config)\n",
    "\n",
    "    # Load CelebA dataset with attribute labels\n",
    "    image_datasets = {\n",
    "        'train': LatentImageDataset(data_dir,\n",
    "                                split='train',\n",
    "                                target_attributes=['Big_Nose']),\n",
    "        'val': LatentImageDataset(data_dir,\n",
    "                                split='val',\n",
    "                                target_attributes=['Big_Nose']),\n",
    "    }\n",
    "    \n",
    "    samplers = {\n",
    "        'train': DistributedSampler(image_datasets['train'], num_replicas=world_size, rank=rank, shuffle=True),\n",
    "        'val': DistributedSampler(image_datasets['val'], num_replicas=world_size, rank=rank, shuffle=False)\n",
    "    }\n",
    "\n",
    "    # Data loaders\n",
    "    dataloaders = {\n",
    "        'train': DataLoader(image_datasets['train'], batch_size=batch_size, sampler=samplers['train'], num_workers=8, pin_memory=True),\n",
    "        'val': DataLoader(image_datasets['val'], batch_size=batch_size, sampler=samplers['val'], num_workers=8, pin_memory=True)\n",
    "    }\n",
    "\n",
    "    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "\n",
    "    print(f'Training dataset size: {dataset_sizes[\"train\"]}')\n",
    "    print(f'Validation dataset size: {dataset_sizes[\"val\"]}')\n",
    "    \n",
    "    \n",
    "    step_count = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        val_accuracies_per_attribute = {attr: 0.0 for attr in condition_config['attribute_condition_config']['attribute_condition_selected_attrs']}\n",
    "        val_tps_per_attribute = {attr: 0.0 for attr in condition_config['attribute_condition_config']['attribute_condition_selected_attrs']}\n",
    "        val_fps_per_attribute = {attr: 0.0 for attr in condition_config['attribute_condition_config']['attribute_condition_selected_attrs']}\n",
    "        val_fns_per_attribute = {attr: 0.0 for attr in condition_config['attribute_condition_config']['attribute_condition_selected_attrs']}\n",
    "        val_tns_per_attribute = {attr: 0.0 for attr in condition_config['attribute_condition_config']['attribute_condition_selected_attrs']}\n",
    "        \n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "            for inputs, labels in tqdm(dataloaders[phase]):\n",
    "                labels = labels.float()                \n",
    "\n",
    "                inputs = inputs.to(rank)\n",
    "                labels = labels.float().to(rank)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):                    \n",
    "                    outputs = model(inputs)\n",
    "\n",
    "\n",
    "\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        step_count += 1\n",
    "\n",
    "                        if (step_count % 10 == 0 or step_count == 1) and wandb is not None and rank == 0:\n",
    "                            wandb.log({'train_loss': loss.item()})\n",
    "\n",
    "                    else:\n",
    "                        # Calculate accuracy per attribute\n",
    "                        for attr_idx, attr in enumerate(condition_config['attribute_condition_config']['attribute_condition_selected_attrs']):\n",
    "                            predicted_labels = torch.sigmoid(outputs) > 0.5\n",
    "\n",
    "                            val_accuracies_per_attribute[attr] += torch.sum(predicted_labels[:, attr_idx] == labels[:, attr_idx])\n",
    "                            # convert to int to avoid overflow\n",
    "                            predicted_labels = predicted_labels.int()\n",
    "                            labels = labels.int()\n",
    "\n",
    "                            val_tps_per_attribute[attr] += torch.sum(predicted_labels[:, attr_idx] & labels[:, attr_idx])\n",
    "                            val_fps_per_attribute[attr] += torch.sum(predicted_labels[:, attr_idx] & ~labels[:, attr_idx])\n",
    "                            val_fns_per_attribute[attr] += torch.sum(~predicted_labels[:, attr_idx] & labels[:, attr_idx])\n",
    "                            val_tns_per_attribute[attr] += torch.sum(~predicted_labels[:, attr_idx] & ~labels[:, attr_idx])\n",
    "\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                \n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f}')            \n",
    "\n",
    "            if phase == 'val' and rank == 0:\n",
    "                # calculate accuracy per attribute\n",
    "                for attr in condition_config['attribute_condition_config']['attribute_condition_selected_attrs']:\n",
    "                    print(f'{attr} Accuracy: {val_accuracies_per_attribute[attr] / dataset_sizes[phase]:.4f}')\n",
    "\n",
    "                if wandb is not None:\n",
    "                    wandb.log({f'{phase}_loss': epoch_loss})\n",
    "                    for attr in condition_config['attribute_condition_config']['attribute_condition_selected_attrs']:\n",
    "                        wandb.log({f'{attr}_accuracy': val_accuracies_per_attribute[attr] / dataset_sizes[phase]})\n",
    "                        #log precision, recall, f1\n",
    "                        precision = val_tps_per_attribute[attr] / (val_tps_per_attribute[attr] + val_fps_per_attribute[attr])\n",
    "                        recall = val_tps_per_attribute[attr] / (val_tps_per_attribute[attr] + val_fns_per_attribute[attr])\n",
    "                        f1 = 2 * precision * recall / (precision + recall)\n",
    "                        wandb.log({f'{attr}_precision': precision})\n",
    "                        wandb.log({f'{attr}_recall': recall})\n",
    "                        wandb.log({f'{attr}_f1': f1})\n",
    "\n",
    "            if rank == 0:\n",
    "                # Save the model\n",
    "                torch.save(model.state_dict(), f'celeba_cnn_latent_nose_classifier_{epoch}_700_ddpm.pth')\n",
    "\n",
    "    if rank == 0:\n",
    "        wandb.finish()\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmegleczmate\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/g/projects/face-diffusion/wandb/run-20241030_084346-12sf80sh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/megleczmate/Face-diffusion/runs/12sf80sh' target=\"_blank\">paranormal-charm-328</a></strong> to <a href='https://wandb.ai/megleczmate/Face-diffusion' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/megleczmate/Face-diffusion' target=\"_blank\">https://wandb.ai/megleczmate/Face-diffusion</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/megleczmate/Face-diffusion/runs/12sf80sh' target=\"_blank\">https://wandb.ai/megleczmate/Face-diffusion/runs/12sf80sh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n",
      "/mnt/g/projects/face-diffusion/dataset/latent_image_dataset.py:77: DtypeWarning: Columns (0,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  attributes_df = pd.read_csv(os.path.join(self.latent_path, 'attributes.csv'))\n",
      "/mnt/g/projects/face-diffusion/dataset/latent_image_dataset.py:77: DtypeWarning: Columns (0,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  attributes_df = pd.read_csv(os.path.join(self.latent_path, 'attributes.csv'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 1800000\n",
      "Validation dataset size: 200000\n",
      "Epoch 1/1\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7032/7032 [34:00<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [03:49<00:00,  3.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 3.3570\n",
      "Big_Nose Accuracy: 0.7425\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f26b76d25cc4b75b034170f1d75cdff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.006 MB of 0.006 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Big_Nose_accuracy</td><td>▁</td></tr><tr><td>Big_Nose_f1</td><td>▁</td></tr><tr><td>Big_Nose_precision</td><td>▁</td></tr><tr><td>Big_Nose_recall</td><td>▁</td></tr><tr><td>train_loss</td><td>█▇▅▂▂▂▁▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Big_Nose_accuracy</td><td>0.7425</td></tr><tr><td>Big_Nose_f1</td><td>0.73455</td></tr><tr><td>Big_Nose_precision</td><td>0.75793</td></tr><tr><td>Big_Nose_recall</td><td>0.71257</td></tr><tr><td>train_loss</td><td>2e-05</td></tr><tr><td>val_loss</td><td>3.35702</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">paranormal-charm-328</strong> at: <a href='https://wandb.ai/megleczmate/Face-diffusion/runs/12sf80sh' target=\"_blank\">https://wandb.ai/megleczmate/Face-diffusion/runs/12sf80sh</a><br/> View project at: <a href='https://wandb.ai/megleczmate/Face-diffusion' target=\"_blank\">https://wandb.ai/megleczmate/Face-diffusion</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241030_084346-12sf80sh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a simple CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # 1st convolutional layer: input 3 channels (RGB), output 16 feature maps\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        # 2nd convolutional layer: input 16 feature maps, output 32 feature maps\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        # 3rd convolutional layer: input 32 feature maps, output 64 feature maps\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        # Max pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        # Fully connected layer: 64*8*8 input size (after pooling), 256 output neurons\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 256)\n",
    "        # Fully connected layer: 256 input neurons, 10 output neurons (for classification into 10 classes)\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Convolution + ReLU + MaxPool layer 1\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        # Convolution + ReLU + MaxPool layer 2\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # Convolution + ReLU + MaxPool layer 3\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        # Flatten the tensor for fully connected layers\n",
    "        x = x.view(-1, 64 * 8 * 8)\n",
    "        # Fully connected layer 1 + ReLU\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # Output layer\n",
    "        x = self.fc2(x)\n",
    "        # final activation function is sigmoid\n",
    "        #x = torch.sigmoid(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "condition_config = {'task_name': 'celeba_attribute_classifier_CNN_Nose',\n",
    "            'condition_types': [ 'attribute' ],\n",
    "            'attribute_condition_config': {\n",
    "                'attribute_condition_num': 1,\n",
    "                'attribute_condition_selected_attrs': ['Big_Nose',]# 'Heavy_Makeup', 'Smiling'],\n",
    "                }\n",
    "            }\n",
    "\n",
    "batch_size = 256\n",
    "learning_rate = 0.001\n",
    "epochs = 1\n",
    "\n",
    "# Initialize the network, optimizer, and loss function\n",
    "model = SimpleCNN()\n",
    "\n",
    "model = model.to('cuda')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "model = train_model(model, criterion, optimizer, condition_config, 1, 0, num_epochs=epochs, batch_size=batch_size, wandb=None)\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model from checkpoint\n",
    "model = SimpleCNN()\n",
    "model.load_state_dict(torch.load('celeba_cnn_latent_smile_classifier_0_700_ddpm.pth'))\n",
    "\n",
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/g/projects/face-diffusion/dataset/latent_image_dataset.py:72: DtypeWarning: Columns (0,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  attributes_df = pd.read_csv(os.path.join(self.latent_path, 'attributes.csv'))\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/mnt/g/data/latents/700/processed/smiling'\n",
    "\n",
    "# Load CelebA dataset with attribute labels\n",
    "val_dataset = LatentImageDataset(data_dir,\n",
    "                                split='val',\n",
    "                                target_attributes=['Smiling'])\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=100, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n"
     ]
    }
   ],
   "source": [
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [04:47<00:00,  6.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 81.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(val_dataloader, total=len(val_dataloader)):\n",
    "        \n",
    "        images = images.to('cuda')\n",
    "        labels = labels.float().to('cuda')\n",
    "        outputs = model(images)\n",
    "        predicted = torch.sigmoid(outputs) > 0.5\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Test Accuracy: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False]], device='cuda:0')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([-56.8299, -56.7637, -56.6909, -56.6829, -56.7782, -56.8402, -56.8629,\n",
       "        -56.8596, -56.8605, -56.8301, -56.8279, -56.8723, -56.9383, -57.0291,\n",
       "        -57.0781, -57.0882, -57.0485, -57.0069, -56.9843, -56.9552, -56.9737,\n",
       "        -57.0442, -57.0857, -57.1444, -57.1491, -57.0995, -57.0527, -57.0481,\n",
       "        -57.0790, -57.1067, -57.1152, -57.1335, -57.1556, -57.1923, -57.2462,\n",
       "        -57.3102, -57.3600, -57.3912, -57.3886, -57.3628, -57.3947, -57.4374,\n",
       "        -57.5014, -57.5159, -57.5236, -57.5355, -57.5644, -57.6168, -57.6549,\n",
       "        -57.6859, -57.7002, -57.6875, -57.6609, -57.6504, -57.6415, -57.6464,\n",
       "        -57.6586, -57.6732, -57.6817, -57.6635, -57.6458, -57.6260, -57.6102,\n",
       "        -57.5267], device='cuda:0'),\n",
       "indices=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0'))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(outputs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0849e-25],\n",
      "        [2.2276e-25],\n",
      "        [2.3958e-25],\n",
      "        [2.4151e-25],\n",
      "        [2.1955e-25],\n",
      "        [2.0635e-25],\n",
      "        [2.0173e-25],\n",
      "        [2.0239e-25],\n",
      "        [2.0221e-25],\n",
      "        [2.0846e-25],\n",
      "        [2.0890e-25],\n",
      "        [1.9984e-25],\n",
      "        [1.8707e-25],\n",
      "        [1.7083e-25],\n",
      "        [1.6267e-25],\n",
      "        [1.6103e-25],\n",
      "        [1.6756e-25],\n",
      "        [1.7467e-25],\n",
      "        [1.7866e-25],\n",
      "        [1.8394e-25],\n",
      "        [1.8057e-25],\n",
      "        [1.6828e-25],\n",
      "        [1.6144e-25],\n",
      "        [1.5223e-25],\n",
      "        [1.5151e-25],\n",
      "        [1.5922e-25],\n",
      "        [1.6686e-25],\n",
      "        [1.6763e-25],\n",
      "        [1.6251e-25],\n",
      "        [1.5808e-25],\n",
      "        [1.5674e-25],\n",
      "        [1.5390e-25],\n",
      "        [1.5053e-25],\n",
      "        [1.4512e-25],\n",
      "        [1.3750e-25],\n",
      "        [1.2897e-25],\n",
      "        [1.2271e-25],\n",
      "        [1.1894e-25],\n",
      "        [1.1924e-25],\n",
      "        [1.2237e-25],\n",
      "        [1.1852e-25],\n",
      "        [1.1357e-25],\n",
      "        [1.0653e-25],\n",
      "        [1.0499e-25],\n",
      "        [1.0419e-25],\n",
      "        [1.0296e-25],\n",
      "        [1.0002e-25],\n",
      "        [9.4919e-26],\n",
      "        [9.1373e-26],\n",
      "        [8.8579e-26],\n",
      "        [8.7325e-26],\n",
      "        [8.8439e-26],\n",
      "        [9.0826e-26],\n",
      "        [9.1783e-26],\n",
      "        [9.2603e-26],\n",
      "        [9.2153e-26],\n",
      "        [9.1031e-26],\n",
      "        [8.9710e-26],\n",
      "        [8.8952e-26],\n",
      "        [9.0585e-26],\n",
      "        [9.2204e-26],\n",
      "        [9.4047e-26],\n",
      "        [9.5544e-26],\n",
      "        [1.0386e-25]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# put activation on the output\n",
    "activation = nn.Sigmoid()\n",
    "print(activation(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 200000\n"
     ]
    }
   ],
   "source": [
    "print(correct, total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stable_diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
