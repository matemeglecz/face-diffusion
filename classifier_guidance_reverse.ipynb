{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import argparse\n",
    "import yaml\n",
    "import os\n",
    "from torchvision.utils import make_grid\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from models.unet_cond_base import Unet\n",
    "from models.vqvae import VQVAE\n",
    "from scheduler.linear_noise_scheduler import LinearNoiseScheduler\n",
    "from scheduler.linear_noise_scheduler_ddim import LinearNoiseSchedulerDDIM\n",
    "from utils.config_utils import *\n",
    "from collections import OrderedDict\n",
    "from datetime import datetime\n",
    "from tools.sample_ddpm_attr_cond import ddim_inversion\n",
    "from torchvision.transforms import Compose, Normalize\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_loss(images, target_color=(0.1, 0.9, 0.5)):\n",
    "    \"\"\"Given a target color (R, G, B) return a loss for how far away on average\n",
    "    the images' pixels are from that color. Defaults to a light teal: (0.1, 0.9, 0.5)\"\"\"\n",
    "    target = torch.tensor(target_color).to(images.device) * 2 - 1  # Map target color to (-1, 1)\n",
    "    target = target[None, :, None, None]  # Get shape right to work with the images (b, c, h, w)\n",
    "    error = torch.abs(images - target).mean()  # Mean absolute difference between the image pixels and the target color\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glasses_loss(x, classifier_model, device='cuda'):\n",
    "    # Create a resnet-18 model\n",
    "    \n",
    "    classifier_model.train()  # Ensure the model is in training mode\n",
    "\n",
    "    # Move the input tensor `x` to the correct device\n",
    "    x = x.to(device)\n",
    "\n",
    "    transforms = Compose([\n",
    "            Normalize(mean=[-0.5047, -0.2201,  0.0777], std=[1.0066, 0.8887, 0.6669])\n",
    "        ])\n",
    "    x = transforms(x)\n",
    "\n",
    "    # Predict the glasses attribute\n",
    "    pred = classifier_model(x)\n",
    "\n",
    "    # Generate a target tensor with the same batch size as the input (assuming a binary classification task)\n",
    "    target = torch.zeros(pred.size(0), 1).to(device)  # Assuming all targets are 1 (glasses present)\n",
    "\n",
    "    # Calculate the loss using Binary Cross Entropy\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "    loss = loss_fn(pred, target)\n",
    "\n",
    "    # Return the loss with gradients enabled\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, classifier_model, cond, scheduler, train_config, diffusion_model_config,\n",
    "           autoencoder_model_config, diffusion_config, dataset_config, vae, use_ddim=False, start_step=0, num_steps=1000, noise_input=None, dir=''):\n",
    "    r\"\"\"\n",
    "    Sample stepwise by going backward one timestep at a time.\n",
    "    We save the x0 predictions\n",
    "    \"\"\"    \n",
    "\n",
    "    # seed random for reproducibility\n",
    "    #torch.manual_seed(9)\n",
    "\n",
    "    im_size = dataset_config['im_size'] // 2 ** sum(autoencoder_model_config['down_sample'])\n",
    "    \n",
    "    ########### Sample random noise latent ##########\n",
    "    if noise_input is not None:\n",
    "        xt = noise_input.to(device)\n",
    "    else:\n",
    "        xt = torch.randn((train_config['num_samples'],\n",
    "                        autoencoder_model_config['z_channels'],\n",
    "                        im_size,\n",
    "                        im_size)).to(device)\n",
    "    ###############################################\n",
    "\n",
    "\n",
    "    ############# Validate the config #################\n",
    "    condition_config = get_config_value(diffusion_model_config, key='condition_config', default_value=None)\n",
    "    assert condition_config is not None, (\"This sampling script is for class conditional \"\n",
    "                                          \"but no conditioning config found\")\n",
    "    condition_types = get_config_value(condition_config, 'condition_types', [])\n",
    "    assert 'attribute' in condition_types, (\"This sampling script is for attribute conditional \"\n",
    "                                          \"but no class condition found in config\")\n",
    "    #validate_class_config(condition_config)\n",
    "    ###############################################\n",
    "    \n",
    "    ############ Create Conditional input ###############\n",
    "    num_classes = condition_config['attribute_condition_config']['attribute_condition_num']\n",
    "    #sample_classes = torch.randint(0, num_classes, (train_config['num_samples'], ))\n",
    "    #print('Generating images for {}'.format(list(sample_classes.numpy())))\n",
    "    cond_input = {\n",
    "        # 'class': torch.nn.functional.one_hot(sample_classes, num_classes).to(device)\n",
    "        #  ['Male', 'Young', 'Bald', 'Bangs', 'Receding_Hairline', 'Black_Hair', 'Blond_Hair', 'Brown_Hair', 'Gray_Hair', 'Straight_Hair', 'Wavy_Hair', 'No_Beard', 'Goatee', 'Mustache', 'Sideburns', 'Narrow_Eyes', 'Oval_Face', 'Pale_Skin', 'Pointy_Nose']\n",
    "        'attribute': cond\n",
    "\n",
    "    }\n",
    "    # Unconditional input for classifier free guidance\n",
    "    uncond_input = {\n",
    "        'attribute': cond_input['attribute'] * 0\n",
    "    }\n",
    "    ###############################################\n",
    "    \n",
    "    # By default classifier free guidance is disabled\n",
    "    # Change value in config or change default value here to enable it\n",
    "    cf_guidance_scale = get_config_value(train_config, 'cf_guidance_scale', 1.0)\n",
    "    \n",
    "    current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    if not use_ddim:\n",
    "        num_steps = diffusion_config['num_timesteps']\n",
    "    \n",
    "    ################# Sampling Loop ########################\n",
    "    for i in tqdm(reversed(range(num_steps - start_step)), total=num_steps):\n",
    "        torch.set_grad_enabled(True)\n",
    "        xt_in = xt.clone()\n",
    "\n",
    "        # activate gradient for xt\n",
    "        xt.requires_grad_(True)\n",
    "        \n",
    "        timestep = ((i-1) * (1000 // num_steps)) + 1\n",
    "        #print(timestep)\n",
    "        \n",
    "        # Get prediction of noise\n",
    "        t = (torch.ones((xt.shape[0],))*timestep).long().to(device)\n",
    "        \n",
    "        \n",
    "        noise_pred_cond = model(xt, t, cond_input)\n",
    "        \n",
    "        if cf_guidance_scale > 1:\n",
    "            noise_pred_uncond = model(xt, t, uncond_input)\n",
    "            noise_pred = noise_pred_uncond + cf_guidance_scale*(noise_pred_cond - noise_pred_uncond)\n",
    "        else:\n",
    "            noise_pred = noise_pred_cond\n",
    "        \n",
    "        # If DDIM is enabled, we need to also compute t_prev for the DDIM reverse process\n",
    "        \n",
    "        if use_ddim:\n",
    "            t_prev = (torch.ones((xt.shape[0],)).to(device) * max(t - (1000 // num_steps), 1)).long().to(device)\n",
    "            xt_new, x0_pred = scheduler.sample_prev_timestep(xt, noise_pred, t, t_prev)  # Use DDIM sampling\n",
    "        else:\n",
    "            xt_new, x0_pred = scheduler.sample_prev_timestep(xt, noise_pred, torch.as_tensor(i).to(device))  # Use DDPM sampling\n",
    "        \n",
    "\n",
    "        #loss = color_loss(x0_pred) * 2\n",
    "        # if not first step, use glasses loss\n",
    "\n",
    "        loss = glasses_loss(x0_pred, classifier_model) * 0.5\n",
    "\n",
    "        # set the loss to require grad\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(i, \"loss:\", loss.item())\n",
    "\n",
    "        cond_grad = -torch.autograd.grad(loss, xt, retain_graph=True)[0] \n",
    "\n",
    "        xt = xt + cond_grad\n",
    "        print(cond_grad.max(), cond_grad.min())\n",
    "\n",
    "        if use_ddim:\n",
    "            t_prev = (torch.ones((xt.shape[0],)).to(device) * max(t - (1000 // num_steps), 1)).long().to(device)\n",
    "            xt, x0_pred = scheduler.sample_prev_timestep(xt, noise_pred, t, t_prev)  # Use DDIM sampling\n",
    "        else:\n",
    "            xt, x0_pred = scheduler.sample_prev_timestep(xt, noise_pred, torch.as_tensor(i).to(device))  # Use DDPM sampling\n",
    "\n",
    "        if i == 0:\n",
    "            # Decode ONLY the final image to save time\n",
    "            ims = vae.decode(xt)\n",
    "        else:\n",
    "            ims = x0_pred\n",
    "        \n",
    "        ims = torch.clamp(ims, -1., 1.).detach().cpu()\n",
    "        ims = (ims + 1) / 2\n",
    "        grid = make_grid(ims, nrow=1)\n",
    "        img = torchvision.transforms.ToPILImage()(grid)\n",
    "\n",
    "        if not os.path.exists(os.path.join(train_config['task_name'], 'cond_attr_samples', dir, current_time)):\n",
    "            os.makedirs(os.path.join(train_config['task_name'], 'cond_attr_samples', dir, current_time), exist_ok=True)\n",
    "        img.save(os.path.join(train_config['task_name'], 'cond_attr_samples', dir, current_time, 'x0_{}.png'.format(i)))\n",
    "        img.close()\n",
    "\n",
    "        # save latent to pt        \n",
    "        torch.save(xt, os.path.join(train_config['task_name'], 'cond_attr_samples', dir, current_time, 'xt_{}.pt'.format(i)))\n",
    "    ##############################################################\n",
    "\n",
    "    return ims, cond_input\n",
    "\n",
    "    \n",
    "def ddim_inversion(scheduler, vae, xt, diffusion_config, condition_input, model, train_config, num_inference_steps=None, dir='', save_img=True):\n",
    "    r\"\"\"\n",
    "    Reverse the process by diffusing the image forward in time.\n",
    "    :param scheduler: the noise scheduler used (e.g., LinearNoiseSchedulerDDIM)\n",
    "    :param vae: the variational autoencoder (VAE) to encode and decode images\n",
    "    :param xt: image tensor that will be diffused forward\n",
    "    :param diffusion_config: configuration for the diffusion process\n",
    "    :param condition_input: the conditioning input for the image\n",
    "    :param model: the diffusion model (e.g., Unet)\n",
    "    :param train_config: the training configuration\n",
    "    \"\"\"\n",
    "\n",
    "    xt = xt.to(device)  # Ensure image is on the correct device\n",
    "    xt = (xt * 2) - 1  # Rescale from [0, 1] to [-1, 1] to match the model's input range\n",
    "\n",
    "    # First, encode the image into latent space using the VAE\n",
    "    z, _ = vae.encode(xt)\n",
    "\n",
    "    all_timesteps = diffusion_config['num_timesteps']\n",
    "\n",
    "    # If the number of inference steps is not provided, use all timesteps\n",
    "    if num_inference_steps is None:\n",
    "        num_timesteps = all_timesteps\n",
    "\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    \n",
    "    intermediate_latents = []\n",
    "    # Move forward in time by applying noise progressively\n",
    "    for i in tqdm(range(1, num_inference_steps), total=num_inference_steps):\n",
    "        t_val = (i * (all_timesteps // num_inference_steps) + 1)\n",
    "        t = (torch.ones((z.shape[0],)) * (i * (all_timesteps // num_inference_steps) + 1)).long().to(z.device)\n",
    "\n",
    "        if i >= num_inference_steps - 1: continue\n",
    "\n",
    "        # Predict noise based on current step and conditions\n",
    "        noise_pred = model(z, t, condition_input)\n",
    "        \n",
    "        next_timestep = t\n",
    "        current_timestep = max(0, t_val - (all_timesteps // num_inference_steps))\n",
    "        current_timestep =  (torch.ones((z.shape[0],)) * current_timestep).long().to(z.device)\n",
    "\n",
    "        # Use the noise prediction to forward-sample to the next timestep using DDIM forward equation\n",
    "        # Reverse the reverse process from sample_prev_timestep\n",
    "        alpha_t = scheduler.alpha_cum_prod.to(z.device)[current_timestep]\n",
    "        alpha_t_next = scheduler.alpha_cum_prod.to(z.device)[next_timestep]\n",
    "        \n",
    "        '''\n",
    "        z_next = (\n",
    "            torch.sqrt(alpha_t_next) * z +\n",
    "            torch.sqrt(1 - alpha_t_next) * noise_pred\n",
    "        )\n",
    "        '''\n",
    "\n",
    "        z_next = (z - torch.sqrt(1 - alpha_t)[0] * noise_pred) * (torch.sqrt(alpha_t_next)[0] / torch.sqrt(alpha_t))[0] + torch.sqrt(1 - alpha_t_next)[0] * noise_pred\n",
    "        \n",
    "        # Optionally, if stochasticity is involved (if ddim_eta > 0), add noise at each step\n",
    "        if scheduler.ddim_eta > 0:\n",
    "            variance = (1 - alpha_t_next) / (1 - alpha_t) * scheduler.betas.to(z.device)[t]\n",
    "            sigma = scheduler.ddim_eta * torch.sqrt(variance)\n",
    "            z_next = z_next + sigma * torch.randn_like(z_next)\n",
    "        \n",
    "        z = z_next  # Move to the next time step\n",
    "\n",
    "        intermediate_latents.append(z)\n",
    "\n",
    "        if save_img:\n",
    "            ims_clamped = torch.clamp(z, -1., 1.).detach().cpu()\n",
    "            ims_clamped = (ims_clamped + 1) / 2  # Rescale to [0, 1]\n",
    "            \n",
    "            # Convert to image and save\n",
    "            grid = make_grid(ims_clamped, nrow=1)\n",
    "            img = torchvision.transforms.ToPILImage()(grid)\n",
    "            \n",
    "            # Save images at each step for visualization\n",
    "            save_dir = os.path.join(train_config['task_name'], 'cond_attr_samples', dir, current_time)\n",
    "            if not os.path.exists(save_dir):\n",
    "                os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "            \n",
    "            # Save the image corresponding to the current timestep\n",
    "            img.save(os.path.join(save_dir, 'x0_{}.png'.format(i)))\n",
    "            img.close()\n",
    "\n",
    "    # convert to torch tensor\n",
    "    intermediate_latents = torch.stack(intermediate_latents, dim=0)\n",
    "\n",
    "    # Return the final noisy latent z and the predicted noise used for the inversion\n",
    "    return intermediate_latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'task_name': 'celebhq-512-64-train-komondor_b', 'continue': True, 'last_step': 0, 'last_epoch': 199, 'dataset_params': {'im_path': 'data/CelebAMask-HQ', 'im_channels': 3, 'im_size': 512, 'name': 'celebhq'}, 'diffusion_params': {'num_timesteps': 1000, 'beta_start': 0.0015, 'beta_end': 0.0195}, 'ldm_params': {'down_channels': [512, 768, 768, 1024], 'mid_channels': [1024, 768], 'down_sample': [True, True, True], 'attn_down': [True, True, True], 'time_emb_dim': 512, 'norm_channels': 32, 'num_heads': 16, 'conv_out_channels': 128, 'num_down_layers': 2, 'num_mid_layers': 2, 'num_up_layers': 2, 'condition_config': {'condition_types': ['attribute'], 'attribute_condition_config': {'attribute_condition_num': 19, 'attribute_condition_selected_attrs': ['Male', 'Young', 'Bald', 'Bangs', 'Receding_Hairline', 'Black_Hair', 'Blond_Hair', 'Brown_Hair', 'Gray_Hair', 'Straight_Hair', 'Wavy_Hair', 'No_Beard', 'Goatee', 'Mustache', 'Sideburns', 'Narrow_Eyes', 'Oval_Face', 'Pale_Skin', 'Pointy_Nose']}}}, 'autoencoder_params': {'z_channels': 3, 'codebook_size': 8192, 'down_channels': [128, 256, 512, 512], 'mid_channels': [512, 512], 'down_sample': [True, True, True], 'attn_down': [False, False, False], 'norm_channels': 32, 'num_heads': 4, 'num_down_layers': 2, 'num_mid_layers': 2, 'num_up_layers': 2}, 'train_params': {'seed': 1111, 'task_name': 'celebhq-512-64-train-komondor_b', 'ldm_batch_size': 1, 'autoencoder_batch_size': 1, 'disc_start': 15000, 'disc_weight': 0.5, 'codebook_weight': 1, 'commitment_beta': 0.2, 'perceptual_weight': 1, 'kl_weight': 5e-06, 'ldm_epochs': 300, 'autoencoder_epochs': 80, 'num_samples': 1, 'num_grid_rows': 1, 'ldm_lr': 1e-06, 'autoencoder_lr': 1e-05, 'autoencoder_acc_steps': 4, 'autoencoder_img_save_steps': 64, 'autoencoder_img_save_steps_wandb': 500, 'autoencoder_log_step_steps_wandb': 500, 'save_latents': True, 'vae_latent_dir_name': 'vae_latents', 'vqvae_latent_dir_name': 'vqvae_latents', 'ldm_ckpt_name': 'ddpm_ckpt.pth', 'vqvae_autoencoder_ckpt_name': 'vqvae_autoencoder_ckpt.pth', 'vae_autoencoder_ckpt_name': 'vae_autoencoder_ckpt.pth', 'vqvae_discriminator_ckpt_name': 'vqvae_discriminator_ckpt.pth', 'vae_discriminator_ckpt_name': 'vae_discriminator_ckpt.pth', 'distributed_data_paralell': True, 'ddpm_log_step_steps_wandb': 500, 'ddpm_img_save_steps_wandb': 1500}, 'sample_params': {'use_ddim': True}}\n",
      "Using DDIM\n",
      "Loaded unet checkpoint\n",
      "Loaded vae checkpoint\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nclassifier_model = torchvision.models.resnet18(pretrained=False)\\n\\nnum_ftrs = classifier_model.fc.in_features\\n\\n# Modify the last fully connected layer for binary classification (1 output)\\nclassifier_model.fc = torch.nn.Linear(num_ftrs, 1)\\n\\n# Load weights from 'celeba_resnet18_latent_glasses_classifier_1.pth'\\nstate = torch.load('celeba_resnet18_latent_glasses_classifier_1.pth', map_location=device)\\n\\nnew_state_dict = OrderedDict()\\nfor k, v in state.items():\\n    name = k[7:]  # remove `module.`\\n    new_state_dict[name] = v\\n\\nclassifier_model.load_state_dict(new_state_dict)\\n\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the config file #\n",
    "with open('celebhq-512-64-train-komondor_b/celeba_komondor_512_b.yaml', 'r') as file:\n",
    "#with open('celebhq-1024-64-16k-komondor/celeba_komondor_16k.yaml', 'r') as file:\n",
    "#with open('celebhq-512-64/celeba_komondor_512.yaml', 'r') as file:\n",
    "    try:\n",
    "        config = yaml.safe_load(file)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "print(config)\n",
    "########################\n",
    "\n",
    "diffusion_config = config['diffusion_params']\n",
    "dataset_config = config['dataset_params']\n",
    "diffusion_model_config = config['ldm_params']\n",
    "autoencoder_model_config = config['autoencoder_params']\n",
    "train_config = config['train_params']\n",
    "sample_config = config['sample_params']\n",
    "\n",
    "########## Create the noise scheduler #############\n",
    "\n",
    "if sample_config['use_ddim']:\n",
    "    print('Using DDIM')\n",
    "    scheduler = LinearNoiseSchedulerDDIM(num_timesteps=diffusion_config['num_timesteps'],\n",
    "                                            beta_start=diffusion_config['beta_start'],\n",
    "                                            beta_end=diffusion_config['beta_end'])\n",
    "else:\n",
    "    scheduler = LinearNoiseScheduler(num_timesteps=diffusion_config['num_timesteps'],\n",
    "                                    beta_start=diffusion_config['beta_start'],\n",
    "                                    beta_end=diffusion_config['beta_end'])\n",
    "###############################################\n",
    "\n",
    "########## Load Unet #############\n",
    "model = Unet(im_channels=autoencoder_model_config['z_channels'],\n",
    "                model_config=diffusion_model_config).to(device)\n",
    "\n",
    "if os.path.exists(os.path.join(train_config['task_name'],\n",
    "                                train_config['ldm_ckpt_name'])):\n",
    "    \n",
    "\n",
    "    ddp_state_dict = torch.load(os.path.join(train_config['task_name'],\n",
    "                                                    train_config['ldm_ckpt_name']),\n",
    "                                        map_location=device)\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in ddp_state_dict.items():\n",
    "        if k.startswith('module.'):\n",
    "            name = k[7:] # remove `module.`\n",
    "        new_state_dict[name] = v\n",
    "    \n",
    "    ddp_state_dict = new_state_dict\n",
    "    print('Loaded unet checkpoint')\n",
    "    model.load_state_dict(ddp_state_dict)\n",
    "else:\n",
    "    raise Exception('Model checkpoint {} not found'.format(os.path.join(train_config['task_name'],\n",
    "                                                                        train_config['ldm_ckpt_name'])))\n",
    "\n",
    "model.train()\n",
    "#####################################\n",
    "\n",
    "# Create output directories\n",
    "if not os.path.exists(train_config['task_name']):\n",
    "    os.mkdir(train_config['task_name'])\n",
    "\n",
    "########## Load VQVAE #############\n",
    "vae = VQVAE(im_channels=dataset_config['im_channels'],\n",
    "            model_config=autoencoder_model_config).to(device)\n",
    "vae.eval()\n",
    "\n",
    "# Load vae if found\n",
    "if os.path.exists(os.path.join(train_config['task_name'],\n",
    "                                train_config['vqvae_autoencoder_ckpt_name'])):\n",
    "    print('Loaded vae checkpoint')\n",
    "\n",
    "    vae_state_dict = torch.load(os.path.join(train_config['task_name'],\n",
    "                                                train_config['vqvae_autoencoder_ckpt_name']),\n",
    "                                    map_location=device)\n",
    "    \n",
    "    \n",
    "    new_state_dict = OrderedDict()\n",
    "\n",
    "    for k, v in vae_state_dict.items():\n",
    "        if k.startswith('module.'):\n",
    "            name = k[7:]\n",
    "        new_state_dict[name] = v   \n",
    "\n",
    "    #new_state_dict = vae_state_dict     \n",
    "    \n",
    "    vae.load_state_dict(new_state_dict, strict=True)\n",
    "else:\n",
    "    raise Exception('VAE checkpoint {} not found'.format(os.path.join(train_config['task_name'],\n",
    "                                                train_config['vqvae_autoencoder_ckpt_name'])))\n",
    "#####################################\n",
    "'''\n",
    "classifier_model = torchvision.models.resnet18(pretrained=False)\n",
    "\n",
    "num_ftrs = classifier_model.fc.in_features\n",
    "\n",
    "# Modify the last fully connected layer for binary classification (1 output)\n",
    "classifier_model.fc = torch.nn.Linear(num_ftrs, 1)\n",
    "\n",
    "# Load weights from 'celeba_resnet18_latent_glasses_classifier_1.pth'\n",
    "state = torch.load('celeba_resnet18_latent_glasses_classifier_1.pth', map_location=device)\n",
    "\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in state.items():\n",
    "    name = k[7:]  # remove `module.`\n",
    "    new_state_dict[name] = v\n",
    "\n",
    "classifier_model.load_state_dict(new_state_dict)\n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_size = dataset_config['im_size'] // 2 ** sum(autoencoder_model_config['down_sample'])\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "noise_input = torch.randn((train_config['num_samples'],\n",
    "                        autoencoder_model_config['z_channels'],\n",
    "                        im_size,\n",
    "                        im_size)).to(device)\n",
    "\n",
    "#noise_input += feature_direction_disentangled.to(device) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [00:00<00:00, 77351.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30000 images\n",
      "Found 0 masks\n",
      "Found 0 captions\n",
      "Found 30000 attributes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from dataset.celeb_dataset import CelebDataset\n",
    "\n",
    "im_dataset = CelebDataset(split='val',\n",
    "                                im_path=dataset_config['im_path'],\n",
    "                                im_size=dataset_config['im_size'],\n",
    "                                im_channels=dataset_config['im_channels'],\n",
    "                                use_latents=False,\n",
    "                                latent_path=os.path.join(train_config['task_name'],\n",
    "                                                         train_config['vqvae_latent_dir_name']),\n",
    "                                #condition_config=temp_conf['condition_config'],\n",
    "                                condition_config=diffusion_model_config['condition_config'],\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_im, sampled_cond = im_dataset[75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "print(sampled_im.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_im = sampled_im.unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_cond = sampled_cond['attribute']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_cond = torch.from_numpy(sampled_cond).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_cond = {'attribute': sampled_cond.to(device)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 19])\n"
     ]
    }
   ],
   "source": [
    "print(sampled_cond['attribute'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 249/250 [00:43<00:00,  5.77it/s]\n"
     ]
    }
   ],
   "source": [
    "# ddim inversion\n",
    "with torch.no_grad():\n",
    "    intermediate_latents = ddim_inversion(scheduler, vae, sampled_im, diffusion_config, sampled_cond, model, train_config, 250, dir=current_time, save_img=True)\n",
    "\n",
    "start_latent = intermediate_latents[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleCNN(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=4096, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.simple_cnn import SimpleCNN\n",
    "\n",
    "classifier_model = SimpleCNN()\n",
    "classifier_model.load_state_dict(torch.load('celeba_cnn_latent_glasses_classifier_0.pth', map_location=device))\n",
    "\n",
    "\n",
    "classifier_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "start_latent = start_latent.detach().clone().to(device)\n",
    "\n",
    "# add gradient to latent\n",
    "start_latent.requires_grad_(True)\n",
    "\n",
    "print(start_latent.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Male',\n",
       " 'Young',\n",
       " 'Bald',\n",
       " 'Bangs',\n",
       " 'Receding_Hairline',\n",
       " 'Black_Hair',\n",
       " 'Blond_Hair',\n",
       " 'Brown_Hair',\n",
       " 'Gray_Hair',\n",
       " 'Straight_Hair',\n",
       " 'Wavy_Hair',\n",
       " 'No_Beard',\n",
       " 'Goatee',\n",
       " 'Mustache',\n",
       " 'Sideburns',\n",
       " 'Narrow_Eyes',\n",
       " 'Oval_Face',\n",
       " 'Pale_Skin',\n",
       " 'Pointy_Nose']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['Male', 'Young', 'Bald', 'Bangs', 'Receding_Hairline', 'Black_Hair', 'Blond_Hair', 'Brown_Hair', 'Gray_Hair', 'Straight_Hair', 'Wavy_Hair', 'No_Beard', 'Goatee', 'Mustache', 'Sideburns', 'Narrow_Eyes', 'Oval_Face', 'Pale_Skin', 'Pointy_Nose']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1000 [00:18<5:13:51, 18.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0., device='cuda:0') tensor(-0., device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1000 [00:28<3:43:00, 13.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0., device='cuda:0') tensor(-0., device='cuda:0')\n",
      "tensor(-0., device='cuda:0') tensor(-0., device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/1000 [00:39<3:24:14, 12.29s/it]"
     ]
    }
   ],
   "source": [
    "#with torch.no_grad():\n",
    "for i in range(1):\n",
    "    ims, cond_trans = sample(model, classifier_model, sampled_cond['attribute'], scheduler, train_config, diffusion_model_config,\n",
    "                autoencoder_model_config, diffusion_config, dataset_config, vae, use_ddim=True, dir=current_time, noise_input=start_latent, num_steps=1000, start_step=0)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stable_diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
